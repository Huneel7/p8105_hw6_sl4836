---
title: "p8105_hw6_sl4836"
author: "Hun"
date: "12/2/2021"
output: github_document
---

```{r, echo =FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(janitor)
library(rstatix)
library(egg)
library(modelr)
library(PerformanceAnalytics)
library(robmed)
```


### Problem 0 - Creating a subdirectory
```{r}
dir.create(file.path(getwd(), "hw6_data_file"), recursive = TRUE)
```

# Problem 1 

### Importing data
```{r, message = FALSE}
birthweight_data <- read_csv("./hw6_data_file/birthweight.csv")
```

### Tidying and wrangling the data
```{r}
cleaned_birthweight_data <-
  birthweight_data %>% 
  janitor::clean_names() %>%
  mutate(across(.cols = c(babysex, frace, malform, mrace), as.factor)) %>%
  mutate(babysex = ifelse(babysex == "1", "male","female"),
         malform = ifelse(malform == "0", "absent","present"),
         frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", 
                        "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
         mrace = recode(mrace, "1" = "White", "2" = "Black", 
                        "3" = "Asian", "4" = "Puerto Rican", "8" = "Other")
         )
```

### Checking Missing Values and the summary of the dataset
```{r}
skimr::skim(cleaned_birthweight_data)

birthweight_data_variables <- names(birthweight_data)
birthweight_data_nrow <- nrow(birthweight_data)
birthweight_data_ncol <- ncol(birthweight_data)
```

There is no missing data. The dimension of the birthweight data is **`r birthweight_data_nrow` x**  **`r birthweight_data_ncol`.** There are **`r birthweight_data_nrow`** number of observations and **`r birthweight_data_ncol`**  variables: *`r birthweight_data_variables`.* 

### Computing Correlation Matrix
```{r, warning = FALSE}
birthweight_data %>% 
  cor_mat() %>%
  cor_gather() %>%
  filter(var1 %in% "bwt") %>%
  filter(!var2 %in% "bwt") %>%
  mutate(
    sig_p = ifelse(p < 0.01, T, F),
    cor_if_sig = ifelse(p < 0.01, cor, NA)
    ) %>% 
  ggplot(aes(
    x = var1, 
    y = var2, 
    fill = cor,
    label = round(cor_if_sig, 2))) + 
  geom_tile(color = "white") +   
  geom_text(
    color = "white",
    size = 4
  ) + 
  scale_x_discrete(
    labels = c("Birth Weight")
  ) + 
  labs(
    x = "Outcome Variable",
    y = "Predictor Variables",
    title = "Correlation Matrix between Predictors and Outcome",
    subtitle = "significant predictors at significance level 0.01",
    fill = "Correlation"
  )

continuous_variables <-  
  cleaned_birthweight_data %>%
  select_if(is.numeric) %>%
  select(-bwt, -ppbmi, -ppwt, -pnumsga, -parity, -pnumlbw) %>%
  colnames() %>% 
  as.vector()
```

  Computing pearson correlation matrix with p-values based on T-test for correlation coefficient. Based on this correlation matrix, I selected continuous variables to be used to fit a scatterplot against the outcome variable in order to check if there is any lienar trend between continuous predictors and the outcome variable. The selected variables are: *`r continuous_variables`.*


### Fitting scatterplots with selected predictors against birthweight to see if there is a linear trend between continuous predictors and the outcome variable
```{r, echo = FALSE, message = FALSE}
continuous_variables <- 
  continuous_variables %>%
  as.list()

for (i in continuous_variables) {
  plot <-
  ggplot(cleaned_birthweight_data, aes_string(i,  "bwt")) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(title = "Scatterplot", y = "Birth Weight") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  print(plot) 
}

```


  It seems that **bhead (baby’s head circumference at birth ) and blength (baby’s length at birth) ** show a strong linear relationship with birth weight and **gaweeks (gestational age in weeks), delwt (mother’s weight at delivery), and wtgain (mother’s weight at delivery)** show a moderate linear relationship with birth weight. These scatterplots confirm the result from the previous correlation matrix. 


### Selecting final continuous independent variables based on the information obtained from the correaltion matrix and scatterplots in order to check correlation between predictors and potential interactions between them. 
```{r}
selected_variables <-
  cleaned_birthweight_data %>%
  select(bhead, blength, delwt, gaweeks, wtgain, bwt)

chart.Correlation(selected_variables, method = "pearson")
```

  This plot serves the purpose of finding potential interactions between predictors. It is to be observed that **bhead and blength** shows a potential interaction effect or multicollinearity and **gaweeks, bhead, and blength** shows a potential interaction effect as well as **wtgain and delwt**.


### Final Selection of predictors including categorical variables to fit linear regression based on the previous pearson correlation matrix. 
```{r}
selected_variables <-
  cleaned_birthweight_data %>%
  select(bhead, blength, gaweeks, bwt, babysex, mrace)
```


### Fitting my model with interaction terms based on the information obtained in the previous parts.
```{r}
fit1 <- lm(bwt ~ bhead + blength + gaweeks + babysex + mrace + bhead:blength +
             bhead:blength:gaweeks, 
           data = selected_variables)

summary(fit1) %>% 
  broom::tidy() %>%
  select(term, estimate, p.value)

summary(fit1) %>% 
  broom::glance()
```

  The reason I chose this model as my final model is because as aforementioned **bhead, blength, and gaweeks** are the most correlated continuous variables with birth weight and **mrace** is also the most correlated categorical variable with birth weight. The reason of including **babysex** is due to hypothesized belief. Plus, all interaction estimates in the model are statistically significant and it has higher adjusted r-squared compared to two given models in the question.

### Fitting the plot of my model residuals against fitted values
```{r, warning = FALSE}
selected_variables %>%
  add_residuals(fit1) %>%
  add_predictions(fit1) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Plot of the model residuals against fitted values",
       x= "Fitted values", y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```

### Computing rmse of models through cross validaiton
```{r}
set.seed(77)

cv_dataset <-
  selected_variables %>% 
  crossv_mc(n = 100,test = 0.2)
  

cv_df <- 
  cv_dataset %>%
   mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df <-
  cv_df %>%
    mutate(
    linear_mod1  = map(train, ~lm(bwt ~ bhead + blength + gaweeks + babysex + mrace 
                                  + bhead:blength + bhead:blength:gaweeks, 
                                  data = .x)),
    linear_mod2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    linear_mod3  = map(train, ~lm(bwt ~ (bhead + blength + babysex)^3, data = .x))
    ) %>%
   mutate(
    rmse_my_model = map2_dbl(linear_mod1, test, ~rmse(model = .x, data = .y)),
    rmse_given_model1 = map2_dbl(linear_mod2, test, ~rmse(model = .x, data = .y)),
    rmse_given_model2 = map2_dbl(linear_mod3, test, ~rmse(model = .x, data = .y))
   )
```


### Fitting the distribution of rmse of the models. 
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_boxplot() +
  labs(title = 
  "Prediction Error Distributions across Models", 
       x = "Models", y = "Root Mean Square Error")  +
  scale_x_discrete(
    labels = c("My Model", "Test Model 1", "Test Model 2")) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

  Here, we are comparing models with respect to the cross-validated prediction error. By and large, my model seems to have the lowest prediction error, followed by test model 2 (with interaction terms) and test model 1 (without interaction terms) when comparing the medians (the line in the middle of the box) and the overall distribution of the box plots.


# Problem 2

### Importing data
```{r}
weather_df <- 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

### Fitting a given model
```{r}
fit0 <- lm(tmax ~ tmin, data = weather_df)
```

### Generating 5000 bootstraps of the dataset
```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

```

### Generating 5000 bootstrap estimates
```{r}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 
```

# Computing log(Beta_0_hat * Beta_1_hat) and getting 5000 of those estiamtes
```{r}
log_betas <-  
  bootstrap_results %>%
  group_by(strap_number) %>%
  summarise(log_betas = log(estimate[1] * estimate[2])) %>%
  select(log_betas, strap_number)
  
```

### Generating 5000 bootstrap estimates
```{r}
bootstrap_results2 <- 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 
```

# Getting 5000 R-Squared estimates
```{r}
r_squared <- 
  bootstrap_results2 %>%
  select(r.squared, strap_number)
  
```

# Fitting density plots of two estimates
```{r, echo = FALSE}
log_betas %>%
  ggplot(aes(x = log_betas)) + geom_density()

r_squared %>%
  ggplot(aes(x = r.squared)) + geom_density()

log_betas %>%
  summarise(log_betas_se = sd(log_betas))

log_betas %>%
  summarise(log_betas_mean = mean(log_betas))

r_squared %>%
  summarise(r_squared_se = sd(r.squared))

r_squared %>%
  summarise(r_squared_mean = mean(r.squared))

```

  It is to be observed that the distribution of $log(\beta_{0} * \beta_{1})$ is approximately normally distributed with mean of 2.01 and 0.024 and the distribution of $\hat{r}^2$ is also approximately normally distributed with mean of 0.91 and 0.0085.
  
# Generating confience interval of two estimates
```{r}
CI_betas <- 
  bootstrap_results %>%
  group_by(term) %>%
  summarize(ci_lower = quantile(estimate, 0.025),
            ci_upper = quantile(estimate, 0.975),
            )

CI_log_betas_lower <- 
  CI_betas %>% 
  summarise(CI_log_betas_lower = log(ci_lower[1] * ci_lower[2]))

CI_log_betas_upper <- 
  CI_betas %>% 
  summarise(CI_log_betas_upper = log(ci_upper[1] * ci_upper[2]))

CI_log_betas <- 
  tibble("95% CI Lower Bound of Log Betas" = as.numeric(CI_log_betas_lower), 
         "95% CI Upper Bound of Log Betas" = as.numeric(CI_log_betas_upper))


CI_log_betas 
```

```{r}
log_betas %>%
  summarize(ci_lower = quantile(log_betas, 0.025),
            ci_upper = quantile(log_betas, 0.975)) %>%
  tibble(
    "95% CI Lower Bound of Log Betas" = as.numeric(ci_lower), 
    "95% CI Upper Bound of Log Betas" = as.numeric(ci_upper)) %>%
  select(-ci_lower, -ci_upper)
```


```{r}
r_squared %>%
  summarize(ci_lower = quantile(r.squared, 0.025),
            ci_upper = quantile(r.squared, 0.975)) %>%
  tibble(
    "95% CI Lower Bound of R-squared" = as.numeric(ci_lower), 
    "95% CI Upper Bound of R-squared" = as.numeric(ci_upper)) %>%
  select(-ci_lower, -ci_upper)
```



